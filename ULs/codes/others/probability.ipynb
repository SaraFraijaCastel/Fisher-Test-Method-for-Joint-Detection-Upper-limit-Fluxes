{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de Significancias Corregidas: [0.11483923236682764, 1.1562329044066577, 0.0, 0.0, 0.22454315788860987, 0.0, 1.6100000000000003, 2.3100000000000005, 0.65, 0.0, 0.58, 1.5800000000000007, 0.0, 2.5500000000000003, 2.22, 0.0, 0.66, 2.72, 1.0499999999999998, 1.0499999999999998, 1.4399999999999997, 2.06, 0.27, 1.0599999999999998, 0.38000000000000006, 0.9, 1.9300000000000006, 0.0, 0.52, 1.7500000000000004, 1.0999999999999999, 0.059999999999999984, 2.87, 0.27, 0.0, 1.0099999999999996, 0.83, 1.6000000000000003, 0.27, 0.7100000000000001, 0.0, 0.84, 0.36999999999999994, -0.14290451464856177, 1.4678098312359305, 0.3644483138821541, 1.0646464477874602, 0.511675146630907, 2.340439936799546, 0.9317313830597742, -0.3701059836132711, 1.8499999999999999, 0.6, 1.0, 0.27, 2.9000000000000004, 0.9799999999999999, 0.0, 2.21, 0.75, 0.7800000000000001, 1.12, 2.19, 2.9700000000000006, 0.7400000000000001, 2.06, 1.0200000000000002, 1.92, 1.2200000000000004, 1.23, 0.9200000000000002, 0.0, 0.9300000000000002, 2.0200000000000005, 2.1799999999999997, 1.7699999999999998, 0.52, 3.4499999999999997, 0.0, 2.2000000000000006, 1.2200000000000004, 0.0, 1.2499999999999993, 1.2600000000000002, 2.4799999999999995, 2.51, 1.9500000000000002, 0.55, 1.2200000000000004, 0.0, 1.3000000000000003]\n",
      "Valor de Chi-cuadrado: 416.3783752098702\n",
      "p-value (normal approx): 3.554299591327729e-20\n",
      "Significancia (normal approx): 9.125986291375758\n",
      "LaTeX file generated and saved as UL_TN/fisher_results_0.3_2.07.tex\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, chi2\n",
    "\n",
    "def significance_to_pvalue(z):\n",
    "    \"\"\"\n",
    "    Convierte un valor de significancia (en sigma) a un p-value (test unidireccional).\n",
    "    \"\"\"\n",
    "    return norm.sf(z)\n",
    "def process_file(filepath, ref_psf):\n",
    "    df = pd.read_csv(filepath)\n",
    "    required = ['Name', 'transit', 'Significance', 'PSF']\n",
    "    for col in required:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"El archivo {filepath} no contiene la columna requerida '{col}'.\")\n",
    "    \n",
    "    data_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        name = row['Name']\n",
    "        transit = row['transit']\n",
    "        significance = row['Significance']\n",
    "        psf = row['PSF']\n",
    "        \n",
    "        p_val = significance_to_pvalue(significance)\n",
    "        pdf_val = norm.pdf(significance)\n",
    "        trials_factor = (psf ** 2) / (ref_psf ** 2)\n",
    "        corrected_p = min(p_val * trials_factor, 1.0)\n",
    "        corrected_sig = norm.isf(corrected_p) if corrected_p < 1.0 else 0.0\n",
    "        \n",
    "        data_list.append((name, transit, significance, corrected_sig, p_val, corrected_p, pdf_val))\n",
    "    return data_list\n",
    "    \"\"\"\n",
    "    Lee el archivo CSV, extrae las columnas 'Name', 'transit', 'Significance' y 'PSF',\n",
    "    y calcula:\n",
    "      - La significancia cruda (la dada en el archivo),\n",
    "      - El p-value crudo (usando norm.sf),\n",
    "      - El p-value corregido (p-value * (PSF²/ref_psf²), acotado a 1.0),\n",
    "      - La significancia corregida (calculada a partir del p-value corregido),\n",
    "      - Y el valor PDF (usando norm.pdf).\n",
    "      \n",
    "    Retorna una lista de tuplas para cada fila:\n",
    "      (Name, Transit, Significance, Corrected Significance, p-value, Corrected p-value, PDF)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Verificar columnas requeridas\n",
    "    required = ['Name', 'transit', 'Significance', 'PSF']\n",
    "    for col in required:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"El archivo {filepath} no contiene la columna requerida '{col}'.\")\n",
    "    \n",
    "    data_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        name = row['Name']\n",
    "        transit = row['transit']\n",
    "        significance = row['Significance']\n",
    "        psf = row['PSF']\n",
    "        \n",
    "        # Calcular p-value crudo y el valor PDF\n",
    "        p_val = significance_to_pvalue(significance)\n",
    "        pdf_val = norm.pdf(significance)\n",
    "        # Calcular factor de trials y p-value corregido\n",
    "        trials_factor = (psf ** 2) / (ref_psf ** 2)\n",
    "        corrected_p = min(p_val * trials_factor, 1.0)\n",
    "        # Calcular la significancia corregida a partir del p-value corregido.\n",
    "        # Si corrected_p es 1, se asume significancia corregida de 0.\n",
    "        if corrected_p >= 1.0:\n",
    "            corrected_sig = 0.0\n",
    "        else:\n",
    "            corrected_sig = norm.isf(corrected_p)\n",
    "        \n",
    "        data_list.append((name, transit, significance, corrected_sig, p_val, corrected_p, pdf_val))\n",
    "    return data_list\n",
    "\n",
    "def fisher_combined(pvalues):\n",
    "    pvalues = np.array(pvalues)\n",
    "    X2 = -2 * np.sum(np.log(pvalues))\n",
    "    dof = 2 * len(pvalues)\n",
    "    p_combined = chi2.sf(X2, dof)\n",
    "    return X2, dof, p_combined\n",
    "\n",
    "\n",
    "def generate_histograms(all_data, output_dir):\n",
    "    \"\"\"\n",
    "    Genera y guarda dos histogramas: uno para las significancias y otro para las significancias corregidas.\n",
    "    Se guardan en la carpeta especificada en output_dir.\n",
    "    \"\"\"\n",
    "    # Extraer los valores\n",
    "    significances = [row[2] for row in all_data]\n",
    "    corrected_significances = [row[3] for row in all_data]\n",
    "    \n",
    "    # Histograma de significancias\n",
    "    plt.figure()\n",
    "    plt.hist(significances, bins=20, edgecolor='black')\n",
    "    plt.xlabel('Significancia')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.ylim([0,5])\n",
    "    plt.title('Histograma de Significancias')\n",
    "    sig_hist_filename = output_dir+'corrected_significance_hist.png'\n",
    "    plt.savefig(sig_hist_filename)\n",
    "    plt.close()\n",
    "    \n",
    "    # Histograma de significancias corregidas\n",
    "    plt.figure()\n",
    "    bins=np.arange(0,3,0.3)\n",
    "    plt.hist(corrected_significances, edgecolor='black',bins=bins)\n",
    "    plt.xlabel('Significancia Corregida')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.ylim([0,5])\n",
    "    plt.title('Histograma de Significancias Corregidas')\n",
    "    corrected_sig_hist_filename = output_dir+'corrected_significance_hist.png'\n",
    "    plt.savefig(corrected_sig_hist_filename)\n",
    "    plt.close()\n",
    "    \n",
    "    return sig_hist_filename, corrected_sig_hist_filename\n",
    "\n",
    "def main(file1, file2, ref_psf, output_dir='UL_TN', output_tex='fisher_results.tex'):\n",
    "    # Crear la carpeta de salida si no existe\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Procesar ambos archivos y combinar los datos\n",
    "    data1 = process_file(file1, ref_psf)\n",
    "    data2 = process_file(file2, ref_psf)\n",
    "    all_data = data1 + data2  # Cada fila: (Name, Transit, Significance, Corr. Significance, p-value, Corr. p-value, PDF)\n",
    "    \n",
    "    # Obtener la lista de significancias corregidas y mostrarla como lista de Python\n",
    "    corrected_significances = [row[3] for row in all_data]\n",
    "    print(\"Lista de Significancias Corregidas:\", corrected_significances)\n",
    "    \n",
    "    # Generar los histogramas y obtener los nombres de los archivos\n",
    "    sig_hist_filename, corrected_sig_hist_filename = generate_histograms(all_data, output_dir+'/')\n",
    "    sig_hist_filename, corrected_sig_hist_filename = generate_histograms(data1, output_dir+'/1st_Transit'+f'PSF_{ref_psf}')\n",
    "    sig_hist_filename, corrected_sig_hist_filename = generate_histograms(data2, output_dir+'/2nd_Transit'+f'PSF_{ref_psf}')\n",
    "    \n",
    "    # Para el test de Fisher usamos los p-values crudos\n",
    "    corrected_significances = [row[3] for row in all_data]\n",
    "    corrected_pvalues = [1 - norm.cdf(sig) for sig in corrected_significances]\n",
    "    \n",
    "    X2, dof, p_combined = fisher_combined(corrected_pvalues)\n",
    "    \n",
    "    # Convertir el valor de chi-cuadrado a p-value y significancia en \"sigmas\" usando la transformación de Wilson-Hilferty.\n",
    "    z_approx = ((X2 / dof)**(1/3) - (1 - 2/(9*dof))) / np.sqrt(2/(9*dof))\n",
    "    normal_p_val = norm.sf(z_approx)\n",
    "    \n",
    "    print(\"Valor de Chi-cuadrado:\", X2)\n",
    "    print(\"p-value (normal approx):\", normal_p_val)\n",
    "    print(\"Significancia (normal approx):\", z_approx)\n",
    "    \n",
    "    critical_value = chi2.ppf(0.95, dof)\n",
    "    decision = \"reject\" if X2 > critical_value else \"do not reject\"\n",
    "    \n",
    "    # Construir la tabla única en LaTeX con todas las columnas.\n",
    "    header = (\"\\\\textbf{GRB} & \\\\textbf{Transit} & \\\\textbf{Significance} & \"\n",
    "              \"\\\\textbf{Corrected Significance} & \\\\textbf{p-value} & \"\n",
    "              \"\\\\textbf{Corrected p-value} & \\\\textbf{PDF} \\\\\\\\ \\\\midrule\\n\")\n",
    "    data_rows = \"\\n\".join([\n",
    "        f\"{name} & {transit} & {sig:.2f} & {corr_sig:.2f} & {p_val:.3e} & {corr_p:.3e} & {pdf_val:.3e} \\\\\\\\\"\n",
    "        for name, transit, sig, corr_sig, p_val, corr_p, pdf_val in all_data\n",
    "    ])\n",
    "    event_table = f\"\"\"\\\\begin{{table}}[h!]\n",
    "\\\\centering\n",
    "\\\\resizebox{{\\\\textwidth}}{{!}}{{%\n",
    "\\\\begin{{tabular}}{{l c c c c c c}}\n",
    "\\\\toprule\n",
    "{header}{data_rows}\n",
    "\\\\bottomrule\n",
    "\\\\end{{tabular}}%\n",
    "}}\n",
    "\\\\caption{{Lista de GRBs con sus Tránsitos, Significancias, Significancias Corregidas, p-values, p-values Corregidos y valores PDF.}}\n",
    "\\\\end{{table}}\n",
    "\"\"\"\n",
    "\n",
    "    # Incluir las imágenes de los histogramas en el documento LaTeX\n",
    "    figures_section = f\"\"\"\\\\begin{{figure}}[h!]\n",
    "\\\\centering\n",
    "\\\\includegraphics[width=0.45\\\\textwidth]{{{os.path.basename(sig_hist_filename)}}}\n",
    "\\\\hfill\n",
    "\\\\includegraphics[width=0.45\\\\textwidth]{{{os.path.basename(corrected_sig_hist_filename)}}}\n",
    "\\\\caption{{Histograma de Significancias (izquierda) y de Significancias Corregidas (derecha).}}\n",
    "\\\\end{{figure}}\n",
    "\"\"\"\n",
    "\n",
    "    conclusion = f\"\"\"\\\\section*{{Conclusion}}\n",
    "Fisher's combined test integrates individual p-values to evaluate a global hypothesis. \n",
    "In this analysis, the test statistic obtained was $X^2 = {X2:.3f}$ with {dof} degrees of freedom. \n",
    "For a significance level of 0.05, the critical chi-square value is {critical_value:.3f}. \n",
    "Since $X^2$ is {\"greater than\" if X2 > critical_value else \"less than or equal to\"} the critical value, \n",
    "we {decision} the null hypothesis that the events are dependet.\n",
    "\\\\newline\n",
    "Additionally, the chi-square value was converted using a normal approximation:\n",
    "\\\\newline\n",
    "Normal p-value: {normal_p_val:.3e} and Normal significance: {z_approx:.2f} sigma.\n",
    "\"\"\"\n",
    "\n",
    "    latex_document = f\"\"\"\\\\documentclass[12pt]{{article}}\n",
    "\\\\usepackage[utf8]{{inputenc}}\n",
    "\\\\usepackage{{booktabs}}\n",
    "\\\\usepackage{{geometry}}\n",
    "\\\\usepackage{{graphicx}}\n",
    "\\\\geometry{{a4paper, margin=1in}}\n",
    "\\\\title{{Results of Fisher's Combined Test}}\n",
    "\\\\author{{Sara Fraija}}\n",
    "\\\\date{{\\\\today}}\n",
    "\\\\begin{{document}}\n",
    "\\\\maketitle\n",
    "\\\\section{{Results}}\n",
    "\n",
    "{event_table}\n",
    "\n",
    "{figures_section}\n",
    "\n",
    "{conclusion}\n",
    "\n",
    "\\\\end{{document}}\n",
    "\"\"\"\n",
    "\n",
    "    # Guardar el documento LaTeX en la carpeta output_dir\n",
    "    tex_output_path = os.path.join(output_dir, output_tex)\n",
    "    with open(tex_output_path, 'w') as f:\n",
    "        f.write(latex_document)\n",
    "    print(\"LaTeX file generated and saved as\", tex_output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spectral_index=2.07\n",
    "    PSF=0.3\n",
    "    file1 = f\"/lustre/hawcz01/scratch/userspace/jorgeamontes/GRB_KN/data/ULs/config/Coordinates/PSF_{PSF}/alfa={spectral_index}/Coordinates_with_Max_Sig_1_0{PSF}.csv\"\n",
    "    file2 = f\"/lustre/hawcz01/scratch/userspace/jorgeamontes/GRB_KN/data/ULs/config/Coordinates/PSF_{PSF}/alfa={spectral_index}/Coordinates_with_Max_Sig_2_0{PSF}.csv\"\n",
    "    main(file1, file2, ref_psf=PSF, output_dir='UL_TN', output_tex=f'fisher_results_{PSF}_{spectral_index}.tex')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( [0.11483923236682764, 1.1562329044066577, 0.0, 0.0, 0.22454315788860987, 0.0, 1.6100000000000003, 2.3100000000000005, 0.65, 0.0, 0.58, 1.5800000000000007, 0.0, 2.5500000000000003, 2.22, 0.0, 0.66, 2.72, 1.0499999999999998, 1.0499999999999998, 1.4399999999999997, 2.06, 0.27, 1.0599999999999998, 0.38000000000000006, 0.9, 1.9300000000000006, 0.0, 0.52, 1.7500000000000004, 1.0999999999999999, 0.059999999999999984, 2.87, 0.27, 0.0, 1.0099999999999996, 0.83, 1.6000000000000003, 0.27, 0.7100000000000001, 0.0, 0.84, 0.36999999999999994, -0.14290451464856177, 1.4678098312359305, 0.3644483138821541, 1.0646464477874602, 0.511675146630907, 2.340439936799546, 0.9317313830597742, -0.3701059836132711, 1.8499999999999999, 0.6, 1.0, 0.27, 2.9000000000000004, 0.9799999999999999, 0.0, 2.21, 0.75, 0.7800000000000001, 1.12, 2.19, 2.9700000000000006, 0.7400000000000001, 2.06, 1.0200000000000002, 1.92, 1.2200000000000004, 1.23, 0.9200000000000002, 0.0, 0.9300000000000002, 2.0200000000000005, 2.1799999999999997, 1.7699999999999998, 0.52, 3.4499999999999997, 0.0, 2.2000000000000006, 1.2200000000000004, 0.0, 1.2499999999999993, 1.2600000000000002, 2.4799999999999995, 2.51, 1.9500000000000002, 0.55, 1.2200000000000004, 0.0, 1.3000000000000003]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 1-2: truncated \\uXXXX escape (880256210.py, line 112)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 112\u001b[0;36m\u001b[0m\n\u001b[0;31m    \\end{{document}}\"\"\")\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 1-2: truncated \\uXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, chi2\n",
    "\n",
    "def significance_to_pvalue(z):\n",
    "    return 1 - norm.cdf(z)\n",
    "\n",
    "def process_file(filepath, ref_psf=0.3):\n",
    "    df = pd.read_csv(filepath)\n",
    "    required = ['Name', 'transit', 'Significance', 'PSF']\n",
    "    for col in required:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"El archivo {filepath} no contiene la columna requerida '{col}'.\")\n",
    "    \n",
    "    data_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        name = row['Name']\n",
    "        transit = row['transit']\n",
    "        significance = row['Significance']\n",
    "        psf = row['PSF']\n",
    "        \n",
    "        p_val = significance_to_pvalue(significance)\n",
    "        pdf_val = norm.pdf(significance)\n",
    "        trials_factor = (psf ** 2) / (ref_psf ** 2)\n",
    "        corrected_p = min(p_val * trials_factor, 1.0)\n",
    "        corrected_sig = norm.isf(corrected_p) if corrected_p < 1.0 else 0.0\n",
    "        \n",
    "        data_list.append((name, transit, significance, corrected_sig, p_val, corrected_p, pdf_val))\n",
    "    return data_list\n",
    "\n",
    "def fisher_combined(pvalues):\n",
    "    pvalues = np.array(pvalues)\n",
    "    X2 = -2 * np.sum(np.log(pvalues))\n",
    "    dof = 2 * len(pvalues)\n",
    "    p_combined = chi2.sf(X2, dof)\n",
    "    return X2, dof, p_combined\n",
    "\n",
    "def generate_histograms(all_data, output_dir):\n",
    "    significances = [row[2] for row in all_data]\n",
    "    corrected_significances = [row[3] for row in all_data]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(significances, bins=20, edgecolor='black')\n",
    "    plt.xlabel('Significancia')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.title('Histograma de Significancias')\n",
    "    plt.savefig(os.path.join(output_dir, 'significance_hist.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    bins = np.arange(0, 3, 0.3)\n",
    "    plt.hist(corrected_significances, edgecolor='black', bins=bins)\n",
    "    plt.xlabel('Significancia Corregida')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.title('Histograma de Significancias Corregidas')\n",
    "    plt.savefig(os.path.join(output_dir, 'corrected_significance_hist.png'))\n",
    "    plt.close()\n",
    "\n",
    "def main(file1, file2, ref_psf=0.3, output_dir='UL_TN', output_tex='fisher_results.tex'):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    data1 = process_file(file1, ref_psf)\n",
    "    data2 = process_file(file2, ref_psf)\n",
    "    all_data = data1 + data2  \n",
    "    \n",
    "    generate_histograms(all_data, output_dir)\n",
    "    \n",
    "    corrected_significances = [row[3] for row in all_data]\n",
    "    corrected_pvalues = [1 - norm.cdf(sig) for sig in corrected_significances]\n",
    "    \n",
    "    X2, dof, p_combined = fisher_combined(corrected_pvalues)\n",
    "    \n",
    "    z_approx = ((X2 / dof)**(1/3) - (1 - 2/(9*dof))) / np.sqrt(2/(9*dof))\n",
    "    normal_p_val = norm.sf(z_approx)\n",
    "    \n",
    "    critical_value = chi2.ppf(0.95, dof)\n",
    "    decision = \"reject\" if X2 > critical_value else \"do not reject\"\n",
    "    \n",
    "    tex_output_path = os.path.join(output_dir, output_tex)\n",
    "    with open(tex_output_path, 'w') as f:\n",
    "        f.write(f\"\"\"\\documentclass[12pt]{{article}}\n",
    "\\usepackage[utf8]{{inputenc}}\n",
    "\\usepackage{{booktabs}}\n",
    "\\usepackage{{geometry}}\n",
    "\\usepackage{{graphicx}}\n",
    "\\geometry{{a4paper, margin=1in}}\n",
    "\\title{{Results of Fisher's Combined Test}}\n",
    "\\author{{Sara Fraija}}\n",
    "\\date{{\\today}}\n",
    "\\begin{{document}}\n",
    "\\maketitle\n",
    "\\section{{Results}}\n",
    "\n",
    "Fisher's Combined Test:\\\\\n",
    "$\\chi^2$: {X2:.3f}\\\\\n",
    "DOF: {dof}\\\\\n",
    "Combined p-value: {p_combined:.3e}\\\\\n",
    "Significance (normal approx): {z_approx:.2f} sigma\\\\\n",
    "Decision: {decision}\n",
    "\n",
    "\\begin{{figure}}[h!]\n",
    "\\centering\n",
    "\\includegraphics[width=0.45\\textwidth]{{significance_hist.png}}\n",
    "\\hfill\n",
    "\\includegraphics[width=0.45\\textwidth]{{corrected_significance_hist.png}}\n",
    "\\caption{{Histograma de Significancias y Significancias Corregidas.}}\n",
    "\\end{{figure}}\n",
    "\n",
    "\\end{{document}}\"\"\")\n",
    "    print(\"LaTeX file generated and saved as\", tex_output_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spectral_index=2.07\n",
    "    PSF=0.15\n",
    "    file1 = f\"/lustre/hawcz01/scratch/userspace/jorgeamontes/GRB_KN/data/ULs/config/Coordinates/PSF_{PSF}/alfa={spectral_index}/Coordinates_with_Max_Sig_1_0{PSF}.csv\"\n",
    "    file2 = f\"/lustre/hawcz01/scratch/userspace/jorgeamontes/GRB_KN/data/ULs/config/Coordinates/PSF_{PSF}/alfa={spectral_index}/Coordinates_with_Max_Sig_2_0{PSF}.csv\"\n",
    "    main(file1, file2, ref_psf=PSF, output_dir='UL_TN', output_tex='fisher_results_{PSF}_{spectral_index}.tex')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05369893 0.01044408 0.25784611 0.5        0.28095731 0.05705343\n",
      " 0.5        0.00538615 0.01320938 0.5        0.25462691 0.0032641\n",
      " 0.5        0.5        0.03215677 0.27425312 0.15865525 0.39358013\n",
      " 0.00186581 0.16354306 0.5        0.01355258 0.22662735 0.21769544\n",
      " 0.13135688 0.30443919 0.14351798 0.5       ]\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "Sig=np.array(Sig)\n",
    "p_values=1-stats.norm.cdf(Sig)\n",
    "Chi2=-2*sum(np.log(p_values))\n",
    "p_value = stats.chi2.sf(Chi2, df)\n",
    "sig=stats.norm.ppf(1-p_value) \n",
    "p_value,sig,Chi2\n",
    "\n",
    "print(np.array(p_values).astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=[1.6100000000000003, 2.3100000000000005, 0.65, 0.0, 0.58, 1.5800000000000007, 0.0, 2.5500000000000003, 2.22, 0.0, 0.66, 2.72, 0.0, 0.0, 1.8499999999999999, 0.6, 1.0, 0.27, 2.9000000000000004, 0.9799999999999999, 0.0, 2.21, 0.75, 0.7800000000000001, 1.12, 0.511675146630907, 1.0646464477874602, 0.0]\n",
    "len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import scipy.stats as stats\n",
    "# from scipy.stats import norm, chi2\n",
    "\n",
    "# def significance_to_pvalue(z):\n",
    "#     \"\"\"\n",
    "#     Convierte un valor de significancia (en sigma) a un p-value (test unidireccional).\n",
    "#     \"\"\"\n",
    "#     return norm.sf(z)\n",
    "\n",
    "# def process_file(filepath, ref_psf=0.3):\n",
    "#     \"\"\"\n",
    "#     Lee el archivo CSV, extrae las columnas 'Name', 'transit', 'Significance' y 'PSF',\n",
    "#     y calcula:\n",
    "#       - La significancia cruda (la dada en el archivo),\n",
    "#       - El p-value crudo (usando norm.sf),\n",
    "#       - El p-value corregido (p-value * (PSF²/ref_psf²), acotado a 1.0),\n",
    "#       - La significancia corregida (calculada a partir del p-value corregido),\n",
    "#       - Y el valor PDF (usando norm.pdf).\n",
    "      \n",
    "#     Retorna una lista de tuplas para cada fila:\n",
    "#       (Name, Transit, Significance, Corrected Significance, p-value, Corrected p-value, PDF)\n",
    "#     \"\"\"\n",
    "#     df = pd.read_csv(filepath)\n",
    "#     # Verificar columnas requeridas\n",
    "#     required = ['Name', 'transit', 'Significance', 'PSF']\n",
    "#     for col in required:\n",
    "#         if col not in df.columns:\n",
    "#             raise ValueError(f\"El archivo {filepath} no contiene la columna requerida '{col}'.\")\n",
    "    \n",
    "#     data_list = []\n",
    "#     for idx, row in df.iterrows():\n",
    "#         name = row['Name']\n",
    "#         transit = row['transit']\n",
    "#         significance = row['Significance']\n",
    "#         psf = row['PSF']\n",
    "        \n",
    "#         # Calcular p-value crudo y el valor PDF\n",
    "#         p_val = significance_to_pvalue(significance)\n",
    "#         pdf_val = norm.pdf(significance)\n",
    "#         # Calcular factor de trials y p-value corregido\n",
    "#         trials_factor = (psf ** 2) / (ref_psf ** 2)\n",
    "#         corrected_p = min(p_val * trials_factor, 1.0)\n",
    "#         # Calcular la significancia corregida a partir del p-value corregido.\n",
    "#         # Si corrected_p es 1, se asume significancia corregida de 0.\n",
    "#         if corrected_p >= 1.0:\n",
    "#             corrected_sig = 0.0\n",
    "#         else:\n",
    "#             corrected_sig = norm.isf(corrected_p)\n",
    "        \n",
    "#         data_list.append((name, transit, significance, corrected_sig, p_val, corrected_p, pdf_val))\n",
    "#     return data_list\n",
    "\n",
    "# def fisher_combined(pvalues):\n",
    "#     \"\"\"\n",
    "#     Combina los p-values usando el método de Fisher.\n",
    "#     Retorna la estadística X^2, los grados de libertad y el p-value combinado.\n",
    "#     \"\"\"\n",
    "#     pvalues = np.array(pvalues)\n",
    "#     # Evitar log(0): reemplazar 0 por un valor muy pequeño\n",
    "#     pvalues[pvalues == 0] = 1e-16\n",
    "#     X2 = -2 * np.sum(np.log(pvalues))\n",
    "#     dof = 2 * len(pvalues)\n",
    "#     p_combined = chi2.sf(X2, dof)\n",
    "#     return X2, dof, p_combined\n",
    "\n",
    "# def generate_histograms(all_data, output_dir):\n",
    "#     \"\"\"\n",
    "#     Genera y guarda dos histogramas: uno para las significancias y otro para las significancias corregidas.\n",
    "#     Se guardan en la carpeta especificada en output_dir.\n",
    "#     \"\"\"\n",
    "#     # Extraer los valores\n",
    "#     significances = [row[2] for row in all_data]\n",
    "#     corrected_significances = [row[3] for row in all_data]\n",
    "    \n",
    "#     # Histograma de significancias\n",
    "#     plt.figure()\n",
    "#     plt.hist(significances, bins=20, edgecolor='black')\n",
    "#     plt.xlabel('Significancia')\n",
    "#     plt.ylabel('Frecuencia')\n",
    "#     plt.ylimit([0,5])\n",
    "#     plt.title('Histograma de Significancias')\n",
    "#     sig_hist_filename = os.path.join(output_dir, 'significance_hist.png')\n",
    "#     plt.savefig(sig_hist_filename)\n",
    "#     plt.close()\n",
    "    \n",
    "#     # Histograma de significancias corregidas\n",
    "#     plt.figure()\n",
    "#     plt.hist(corrected_significances, bins=20, edgecolor='black')\n",
    "#     plt.xlabel('Significancia Corregida')\n",
    "#     plt.ylabel('Frecuencia')\n",
    "#     plt.title('Histograma de Significancias Corregidas')\n",
    "#     corrected_sig_hist_filename = os.path.join(output_dir, 'corrected_significance_hist.png')\n",
    "#     plt.savefig(corrected_sig_hist_filename)\n",
    "#     plt.close()\n",
    "    \n",
    "#     return sig_hist_filename, corrected_sig_hist_filename\n",
    "\n",
    "# def main(file1, file2, ref_psf=0.3, output_dir='UL_TN', output_tex='fisher_results.tex'):\n",
    "#     # Crear la carpeta de salida si no existe\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "    \n",
    "#     # Procesar ambos archivos y combinar los datos\n",
    "#     data1 = process_file(file1, ref_psf)\n",
    "#     data2 = process_file(file2, ref_psf)\n",
    "#     all_data = data1 + data2  # Cada fila: (Name, Transit, Significance, Corr. Significance, p-value, Corr. p-value, PDF)\n",
    "    \n",
    "#     # Obtener la lista de significancias corregidas y mostrarla como lista de Python\n",
    "#     corrected_significances = [row[3] for row in all_data]\n",
    "#     print(\"Lista de Significancias Corregidas:\", corrected_significances)\n",
    "    \n",
    "#     # Generar los histogramas y obtener los nombres de los archivos\n",
    "#     sig_hist_filename, corrected_sig_hist_filename = generate_histograms(all_data, output_dir)\n",
    "    \n",
    "#     # Para el test de Fisher usamos los p-values crudos\n",
    "#     all_pvals = [row[4] for row in all_data]\n",
    "#     X2, dof, p_combined = fisher_combined(all_pvals)\n",
    "    \n",
    "#     # Método alternativo: Convertir el estadístico chi-cuadrado a p-value y luego a significancia normal\n",
    "#     # Usando la función survival de chi2 y la función cuantil de la normal:\n",
    "#     p_value_chi2 = stats.chi2.sf(X2, dof)\n",
    "#     z_value = norm.ppf(1 - p_value_chi2)\n",
    "    \n",
    "#     print(\"Valor de Chi-cuadrado:\", X2)\n",
    "#     print(\"p-value (chi2):\", p_value_chi2)\n",
    "#     print(\"Significancia (normal):\", z_value)\n",
    "    \n",
    "#     critical_value = chi2.ppf(0.95, dof)\n",
    "#     decision = \"reject\" if X2 > critical_value else \"do not reject\"\n",
    "    \n",
    "#     # Construir la tabla única en LaTeX con todas las columnas.\n",
    "#     header = (\"\\\\textbf{GRB} & \\\\textbf{Transit} & \\\\textbf{Significance} & \"\n",
    "#               \"\\\\textbf{Corrected Significance} & \\\\textbf{p-value} & \"\n",
    "#               \"\\\\textbf{Corrected p-value} & \\\\textbf{PDF} \\\\\\\\ \\\\midrule\\n\")\n",
    "#     data_rows = \"\\n\".join([\n",
    "#         f\"{name} & {transit} & {sig:.2f} & {corr_sig:.2f} & {p_val:.3e} & {corr_p:.3e} & {pdf_val:.3e} \\\\\\\\\"\n",
    "#         for name, transit, sig, corr_sig, p_val, corr_p, pdf_val in all_data\n",
    "#     ])\n",
    "#     event_table = f\"\"\"\\\\begin{{table}}[h!]\n",
    "# \\\\centering\n",
    "# \\\\resizebox{{\\\\textwidth}}{{!}}{{%\n",
    "# \\\\begin{{tabular}}{{l c c c c c c}}\n",
    "# \\\\toprule\n",
    "# {header}{data_rows}\n",
    "# \\\\bottomrule\n",
    "# \\\\end{{tabular}}%\n",
    "# }}\n",
    "# \\\\caption{{Lista de GRBs con sus Tránsitos, Significancias, Significancias Corregidas, p-values, p-values Corregidos y valores PDF.}}\n",
    "# \\\\end{{table}}\n",
    "# \"\"\"\n",
    "\n",
    "#     # Incluir las imágenes de los histogramas en el documento LaTeX\n",
    "#     figures_section = f\"\"\"\\\\begin{{figure}}[h!]\n",
    "# \\\\centering\n",
    "# \\\\includegraphics[width=0.45\\\\textwidth]{{{os.path.basename(sig_hist_filename)}}}\n",
    "# \\\\hfill\n",
    "# \\\\includegraphics[width=0.45\\\\textwidth]{{{os.path.basename(corrected_sig_hist_filename)}}}\n",
    "# \\\\caption{{Histograma de Significancias (izquierda) y de Significancias Corregidas (derecha).}}\n",
    "# \\\\end{{figure}}\n",
    "# \"\"\"\n",
    "\n",
    "#     conclusion = f\"\"\"\\\\section*{{Conclusion}}\n",
    "# Fisher's combined test integrates individual p-values to evaluate a global hypothesis. \n",
    "# In this analysis, the test statistic obtained was $X^2 = {X2:.3f}$ with {dof} degrees of freedom. \n",
    "# For a significance level of 0.05, the critical chi-square value is {critical_value:.3f}. \n",
    "# Since $X^2$ is {\"greater than\" if X2 > critical_value else \"less than or equal to\"} the critical value, \n",
    "# we {decision} the null hypothesis that the events are independent.\n",
    "# \\\\newline\n",
    "# Using the chi-square distribution, we calculated:\n",
    "# \\\\newline\n",
    "# p-value: {p_value_chi2:.2e} \n",
    "# \\\\newline\n",
    "# Normal significance (z-value): {z_value:.2f} sigma.\n",
    "# \"\"\"\n",
    "\n",
    "#     latex_document = f\"\"\"\\\\documentclass[12pt]{{article}}\n",
    "# \\\\usepackage[utf8]{{inputenc}}\n",
    "# \\\\usepackage{{booktabs}}\n",
    "# \\\\usepackage{{geometry}}\n",
    "# \\\\usepackage{{graphicx}}\n",
    "# \\\\geometry{{a4paper, margin=1in}}\n",
    "# \\\\title{{Results of Fisher's Combined Test}}\n",
    "# \\\\author{{Sara Fraija}}\n",
    "# \\\\date{{\\\\today}}\n",
    "# \\\\begin{{document}}\n",
    "# \\\\maketitle\n",
    "# \\\\section{{Results}}\n",
    "\n",
    "# {event_table}\n",
    "\n",
    "# {figures_section}\n",
    "\n",
    "# {conclusion}\n",
    "\n",
    "# \\\\end{{document}}\n",
    "# \"\"\"\n",
    "\n",
    "#     # Guardar el documento LaTeX en la carpeta output_dir\n",
    "#     tex_output_path = os.path.join(output_dir, output_tex)\n",
    "#     with open(tex_output_path, 'w') as f:\n",
    "#         f.write(latex_document)\n",
    "#     print(\"LaTeX file generated and saved as\", tex_output_path)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     file1 = \"/lustre/hawcz01/scratch/userspace/jorgeamontes/GRB_KN/data/ULs/config/Coordinates/Coordinates_with_Max_Sig_1_00.3.csv\"\n",
    "#     file2 = \"/lustre/hawcz01/scratch/userspace/jorgeamontes/GRB_KN/data/ULs/config/Coordinates/Coordinates_with_Max_Sig_2_00.3.csv\"\n",
    "#     main(file1, file2, ref_psf=0.3, output_dir='UL_TN', output_tex='fisher_results.tex')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0805629053798605e-10"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1=16.1\n",
    "e2=158.7\n",
    "alfa=2.07\n",
    "E0=1\n",
    "N0=2.07e-10\n",
    "N0=3.88e-11\n",
    "1.6*N0/(E0**-alfa)*(1/(-alfa+2))*(e2**(-alfa+2)-e1**(-alfa+2))                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.301752845280249e-10 3.34e-10 16.1 158.7\n"
     ]
    }
   ],
   "source": [
    "def non_to_int2(E1, E2, flux, alpha, pivot):\n",
    "    \"\"\"Convierte un flujo a un valor integrado según el índice espectral.\"\"\"\n",
    "    A = flux / (pivot ** (-alpha))\n",
    "    if alpha > 2:\n",
    "        F1 = (E2 ** (2 - alpha)) / (2 - alpha)\n",
    "        F2 = (E1 ** (2 - alpha)) / (2 - alpha)\n",
    "        return 1.6 * A * (F1 - F2)\n",
    "    elif alpha == 2:\n",
    "        F3 = np.log(E2)\n",
    "        F4 = np.log(E1)\n",
    "        return A * (F3 - F4)\n",
    "    else:\n",
    "        return None\n",
    "Frans=pd.read_csv('/lustre/hawcz01/scratch/userspace/jorgeamontes/GRB_KN/data/ULs/codes/data.csv')\n",
    "a=non_to_int2(e1,e2,N0,alfa,1)\n",
    "E=pd.read_csv('/lustre/hawcz01/scratch/userspace/jorgeamontes/GRB_KN/data/ULs/config/Energy_ranges_2.07.csv',names=['Name','E1','E2'])\n",
    "UB=Frans['upperBound_2ndT'][6];E1=E['E1'][6];E2=E['E2'][6]\n",
    "UL=non_to_int2(E1,E2,UB,alfa,1)\n",
    "\n",
    "# UB=Frans['upperBound'];E1=E['E1'];E2=E['E2']\n",
    "# UL=non_to_int2(E1.values,E2.values,UB.values,alfa,1)\n",
    "# \n",
    "print(UL,UB,E1,E2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
